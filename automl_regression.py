# -*- coding: utf-8 -*-
"""AutoML_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C6me5Q-GMVfEnnF8keeA_fSiqFqXPyYi
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

import scipy
from sklearn.metrics import r2_score
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression 
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import VotingRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import AdaBoostRegressor
import xgboost as xg


from sklearn.metrics import accuracy_score,confusion_matrix,classification_report  #https://medium.com/@MohammedS/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b
from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV
from sklearn.feature_selection import mutual_info_regression
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression

from sklearn.feature_selection import VarianceThreshold
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.compose import make_column_transformer 
from sklearn.impute import SimpleImputer
from sklearn.experimental import enable_iterative_imputer  # noqa
from sklearn.impute import IterativeImputer
from scipy.stats import shapiro,chi2_contingency
from sklearn.pipeline import make_pipeline

# %matplotlib inline
#pd.set_option('display.max_colwidth',100)

"""# To import our own module in google colab

## https://colab.research.google.com/github/bebi103a/bebi103a.github.io/blob/master/lessons/03/packages_and_modules.ipynb#scrollTo=ojMnEc6NH2lO
"""

class DataProcess:
    def __init__(self, df_url, type_of_problem):  # df_url can also be path.
        self.df_url = df_url                      
        self.problem_type =type_of_problem
        self.dataframe = self.get_df(df_url)
        self.nan_threshold()
        self.X,self.y = self.get_X_and_y(self.dataframe)
        self.out_filtered = OutlierRemoval(self.dataframe,self.X,self.y)

    def nan_threshold(self):
      for  col in list(self.dataframe.columns):
        if self.dataframe[col].isnull().sum() > round(self.dataframe.shape[0]*.3):
          print(f'{col} column has more than 30% of nan values so we remove them')
          self.dataframe.drop(columns=col,inplace=True)
        
    def get_df(self,df_url):
        dataset_format = self.df_url[-1:-4:-1]
        dataset_format = dataset_format[::-1]
        try:
            if dataset_format == 'csv':
                return pd.read_csv(df_url)
            elif dataset_format == 'tsv':
                return pd.read_table(df_url)
            elif dataset_format == 'txt':
                return pd.read_table(df_url)
            elif dataset_format == 'xlxs':
                return pd.read_excel(df_url)
            else:
                print('Dataset is not in recognisable format')
                
        except FileNotFoundError:
            print('File Not Found')
    def get_X_and_y(self,dataframe):
      flag=0
      try:
          X = dataframe.drop(dataframe.iloc[:,-1].name,axis='columns')
          y = dataframe.iloc[:,-1]
          print('feature and target separated')
          return X,y
      except :
            flag +=1
            print('Cannot load dataset :(') # If the url, path is invalid or format is not recognizable  .
            exit(0)
      finally:
          if flag == 1:
              X = pd.DataFrame()
              y = pd.DataFrame()
              return X,y

class OutlierRemoval :
  def __init__(self,df,X,y):
    self.df = df
    self.X = X
    self.y = y
    self.cat_features = X.select_dtypes(include='object')
    self.cont_features = X.select_dtypes(exclude='object')
    print('select dtypes cont',self.cont_features.columns)
    print('select dtypes cat',self.cat_features.columns)

    print('raw X with nan ',self.X.isna().sum())
   
    self.X,self.cat_colname,self.cont_colname = self.impute_nan()
    self.new_cat_colname = self.cat_colname.copy()
    print('returned df form impute X null values and nuniques',self.X.isna().sum(),self.X.nunique())
    print('returned df form impute x category column names ',self.cat_colname)
    print('returned df form impute x NEW category column names ',self.new_cat_colname)
    print('returned df form impute x continuous column names ',self.cont_colname)
    
    self.normal_cont_dict,self.not_normal_cont_dict = self.outlier_continuous()
    print('normal_cont_dict',self.normal_cont_dict)
    print('not_normal_cont_dict',self.not_normal_cont_dict)

    self.remove_cont_not_normal_outlier()
    print('after removal of cout_not_normal_outlier',self.df.describe())
    self.remove_cont_not_normal_outlier()

    print('shape of X and y',self.X.shape,self.y.shape)
    print(self.X.isnull().sum())
    print(self.y.isnull().sum())

    self.data = pd.concat([self.X,self.y],axis=1)
    print('Combined X and y as data',self.data.isnull().sum(),self.data.shape)
    print('out cat cols',self.cat_colname) 
    print('out new cat cols',self.new_cat_colname) 
    print('out cont cols',self.cont_colname) 
    print('=======================================================')
    self.fs_regression = FeatureEngineeringAndSelection(self.X, self.y, self.cat_colname, self.new_cat_colname, self.cont_colname)

  def impute_nan(self): # previously it was X
    cont_col = list(self.cont_features.columns)
    cat_col = list(self.cat_features.columns)
    to_change = []
    for name in cont_col:
      if self.X[name].nunique() < 10:
        if self.X[name].nunique() == 1:
          continue
        to_change.append(name)

    for i in to_change:
      cont_col.remove(i)
    cat_col.extend(to_change)

    ct = make_column_transformer((IterativeImputer(),cont_col),
                         (SimpleImputer(strategy='most_frequent'),cat_col),
                           remainder='passthrough')
    arr = ct.fit_transform(self.X)
    df_X = pd.DataFrame(arr,columns=cont_col+cat_col)
    return df_X,cat_col,cont_col


  def outlier_continuous(self):
    normality_dict = dict()
    not_normality_dict = dict()
    
    for name in self.cont_colname:
      stat, p = shapiro(self.X[name])
      if p > 0.05:
        normality_dict[name] = stat,p
      else:
        not_normality_dict[name] = stat,p
        

    for name in not_normality_dict.keys():   # If a feature is not normally distributed and it contains nuiques < 15, then it is categorical.
      if self.X[name].nunique() < 15:  # these are categorical features
        self.new_cat_colname.extend(name)
        # self.cat_features[name] = self.cont_features[name].copy()
        # self.cont_features.drop(columns=[name],inplace=True)
      
    for var in self.cat_colname:
      if var in not_normality_dict.keys():
        del not_normality_dict[var]

    return normality_dict,not_normality_dict

  def remove_cont_not_normal_outlier(self):
    if (len(self.not_normal_cont_dict) != 0):
      for key,_ in self.not_normal_cont_dict.items():
        Q1 = self.df[key].quantile(.25)
        Q3 = self.df[key].quantile(.75)
        IQR = Q3 - Q1
        lower_limit = Q1 - (1.5 * IQR)
        upper_limit = Q3 + (1.5 * IQR)
        self.df[key] = np.where(self.df[key] > upper_limit, upper_limit,
                                (np.where(self.df[key] < lower_limit,lower_limit,self.df[key]))) 
        
  def remove_cont_normal_outlier(self):
    if(len(self.normal_cont_dict) != 0):
      for key,_ in self.normal_cont_dict.items():
        mean_upper_limit = self.cont_features[key].mean() + (3 * self.cont_features[key].std())
        mean_lower_limit = self.cont_features[key].mean() - (3 * self.cont_features[key].std())
        self.df[key] =  self.cont_features[(self.cont_features[key] > mean_lower_limit) & (self.cont_features[key] < mean_upper_limit)]

class FeatureEngineeringAndSelection:
  def __init__(self, X, y, cat_colname, new_cat_colname, cont_colname):
    self.X = X
    self.y = y
    self.cat_colname = cat_colname
    self.new_cat_colname = new_cat_colname
    self.cont_colname = cont_colname
    self.X[self.cont_colname] = self.X[self.cont_colname].astype('float')
    self.zero_variance()
    self.N = self.select_N(self.cont_colname)
    print('dtypes of X ',self.X.dtypes)
    
    print('After 0 var zero variance drop',self.X.columns)
    print('After 0 var cat column names',self.cat_colname)
    print('After 0 var new cat column names',self.new_cat_colname)
    print('After 0 var cont column names',self.cont_colname)
    print('X column names',self.X.columns)
    self.cor_within_features_to_drop = self.feature_selection_cont_within_features(0.85)
    print('After correlation ',self.X.columns)
    print('After correlation dytpes of X',self.X.dtypes)
    print('returned cor features to drop from correlation',self.cor_within_features_to_drop)
    print('After highly correlated features removed, remainging feature names in cont_colname',self.cont_colname)

    self.mutual_series,self.top_4_mutual_info = self.feature_selection_cont_with_target()
    print('mutual info series returned',self.mutual_series)
    print('top N mutual info series returned',self.top_4_mutual_info)

    self.top_4_extra_tree_classif = self.feature_selection_cont_f_regression_with_target()
    self.high_corr_with_target = self.feature_selection_cont_cor_with_target()
    print('cont correlation with target',self.high_corr_with_target)

    self.top_ten_cat_col_value = self.choose_top_ten_cat_values()
    #print(self.encoding_and_scaling())

    print('FINALLY, BEFORE CONT FEATURE SELECTION X IS',self.X[self.cont_colname].columns)
    print('highly correlated with target ',self.high_corr_with_target,type(self.high_corr_with_target))
    print('top N mutual info classif series ',self.top_4_mutual_info,type(self.top_4_mutual_info))
    print('top N extra tress clasifier feature importance ',self.top_4_extra_tree_classif,type(self.top_4_extra_tree_classif))

    self.all_cont_features_selected = ((self.high_corr_with_target + self.top_4_mutual_info) + self.top_4_extra_tree_classif)
    print('All cont features selected ',self.all_cont_features_selected)


    self.final_cont_feature_to_consider = set(self.all_cont_features_selected)
    print('final cont features selected ',self.final_cont_feature_to_consider)
    print(' N   VALUE = ',self.N)
    print(' NO OF CONT COLUMNS = ',len(self.cont_colname))
    print('remaining cat columns which are to be feature selected ',self.cat_colname)
    print('FINALLY, BEFORE CONT FEATURE SELECTION X IS',self.X[self.cont_colname].columns)
    print('FINALLY, After CONT FEATURE SELECTION X IS',self.X[self.final_cont_feature_to_consider].columns)

    self.ordinal_encode_cat()
    self.N_CAT = self.select_N(self.cat_colname)
    print(f'CAT N {self.N_CAT}, cont N{self.N}')
    self.cat_highly_correlated  = self.feature_selection_cat_corr_with_target()
    self.mut_reg, self.top_n_mut_reg_cat = self.feature_selection_cat_mut_with_target()

    self.all_cat_features_to_consider = set(self.cat_highly_correlated + self.top_n_mut_reg_cat)
    self.all_features_selected = list(self.all_cat_features_to_consider) + list(self.final_cont_feature_to_consider)
 
    print('mutual info series cat returned',self.mut_reg)
    print('top 4 mutual info series cat returned',self.top_n_mut_reg_cat)
    print('all cat features to consider ',self.all_cat_features_to_consider)
    print('all cont features to consider ',self.final_cont_feature_to_consider)
    print('all features selected ',self.all_features_selected)

    print('all cont features before feature selection',self.cont_colname)
    print('all cat features before feature selection',self.cat_colname)

    self.cat_selected, self.cont_selected = self.select_con_cat() 
    print(f'cat selected {self.cat_selected} , cont selected {self.cont_selected}')
    print('target',self.y.name)


    self.X_scaled = self.encoding_and_scaling()
    print('X_scaled columns',self.X_scaled.columns)
    print(self.X_scaled.head(2))
    # self.top_n_extra_tree_classif_cat = self.extra_tree_classifier_cat()
    # print('top n extra tree classif cat ',self.top_n_extra_tree_classif_cat)
     
    self.model_score = BuildModel(self.X_scaled,self.y)
    print(self.model_score.best_model)
    
  def select_N(self,colname):
    N = 0
    if len(colname) <= 3:
      N = 1
    elif len(colname) <= 5:
      N = 2
    elif len(colname) <=10:
      N = 4
    elif len(colname) <= 15:
      N = 6
    elif len(colname) <= 20:
      N = 10
    else:
      N = 15
    return N

  def zero_variance(self):
    if len(self.cont_colname) != 0:
      var_threshold = VarianceThreshold(threshold=0)
      var_threshold.fit_transform(self.X[self.cont_colname])
      drop_features = self.X[self.cont_colname].columns[~var_threshold.get_support()]  #df[cont.columns].columns[~var.get_support()]
      print('zero_variance columns to drop',drop_features)

      if len(drop_features) != 0:
        self.X.drop(columns=drop_features,inplace=True)
        print('inside if drop_feature')
        print('zero var feat',drop_features)
        for i in drop_features:
          self.cont_colname.remove(i)
      print('After drop zero variance col, remaining cols in cont are',self.cont_colname)
      


  # using correlation
  def feature_selection_cont_within_features(self,thershold):
    cor_matrix = self.X.corr(method='pearson')
    cor_features_drop = set()
    for i in range(len(cor_matrix.columns)):
      for j in range(i):
        if abs(cor_matrix.iloc[i,j]) > thershold:
          cor_features_drop.add(cor_matrix.columns[i]) 
    print('Highly correlated features',cor_features_drop)

    if len(cor_features_drop) != 0:
      self.X.drop(columns=cor_features_drop,inplace=True)
      for col in cor_features_drop:
        print('cont name to be removed form cont_colname',col)
        self.cont_colname.remove(col)
     
    return cor_features_drop


  # using mutual info reg
  def feature_selection_cont_with_target(self):
    top_n_mutual_info = list()
    print('////////////////////cont_col',self.cont_colname)
    mutual_info = mutual_info_regression(self.X[self.cont_colname],self.y)
    mutual_info_series = pd.Series(mutual_info)
    mutual_info_series.index = self.cont_colname
    mutual_info_series = mutual_info_series.sort_values(ascending=False)

    if len(mutual_info_series) > 10:
      mutual_info_series = mutual_info_series[:10]
    top_n_mutual_info = mutual_info_series[:self.N].copy().index  
    return mutual_info_series,list(top_n_mutual_info)


  # using correlation of cont with target
  def feature_selection_cont_cor_with_target(self):
    cor_with_target_dict = dict()
    to_drop = set()
    for col in self.cont_colname:
      corr = self.X[col].corr(self.y)
      print(f'correlation of feature with target {col} : {corr}')
      if corr >= abs(.2):
        cor_with_target_dict[col] = corr
      else:
        to_drop.add(col)
    print('to_drop that are weakly correlated with target',to_drop)
    print('correlated with target above the threshold .2 ',cor_with_target_dict)
    high_corr_with_target = list(cor_with_target_dict.keys())
    print('highly correlated with target ',high_corr_with_target)
    # if len(to_drop) != 0:
    #   for i in to_drop:
    #     self.X.drop(columns=i,inplace=True)
    #     self.cont_colname.remove(i)

    return high_corr_with_target


  # for cont features # https://machinelearningmastery.com/feature-selection-for-regression-data/
  def feature_selection_cont_f_regression_with_target(self):
    f_scores_dict = dict()
    model = SelectKBest(score_func=f_regression, k='all')
    model.fit_transform(self.X[self.cont_colname],self.y)
    for colname,fscore in zip(self.cont_colname, model.scores_):
      f_scores_dict[colname] = fscore

    print('f_scores',model.scores_)
    print('f_scores_dict',f_scores_dict)

    f_scores_dict = dict(sorted(f_scores_dict.items(),key= lambda x: x[1],reverse=True))
    print('SORTED f_scores_dict',f_scores_dict)
    return list()
    # model = ExtraTreesClassifier()
    # model.fit(self.X[self.cont_colname],self.y)
    # arr = model.feature_importances_
    # feat_imp = {col:val for col,val in zip(self.cont_colname,arr)}
    # print('feature importance',feat_imp)
    # feat_imp = dict(sorted(feat_imp.items(),key=lambda x: x[1],reverse=True))
    # print('SORTED feature importance',feat_imp)
    # top_n_extra_tree_classif = list(feat_imp.keys())
    # top_n_extra_tree_classif = top_n_extra_tree_classif[:self.N]
    # print('top4_extra_tree_classif',top_n_extra_tree_classif)
    # return top_n_extra_tree_classif

    # print('feature importance array------',sorted(arr,reverse=True))

  def choose_top_ten_cat_values(self):
    top_ten_col_value = dict()
    print('cat colnames choose top ten from each',self.cat_colname)
    for col in self.X[self.cat_colname]:
      print(col,' : ',len(self.X[col].unique()),'label')

    for col in self.cat_colname:
      top_ten = [x for x in self.X[col].value_counts().sort_values(ascending=False).head(12).index]
      print(f'top 10 for {col} ',top_ten)
      top_ten_col_value[col] = top_ten
    print(top_ten_col_value)
    return top_ten_col_value

  

  
  # def ordinal_encode_cat(self):
  #   n_unique_series = self.X[self.cat_colname].nunique()
  #   print(n_unique_series)
  #   for colname,unique in zip(n_unique_series.index,n_unique_series):
  #     if unique <= 10:
  #       oe = OrdinalEncoder()
  #       self.X[colname] = oe.fit_transform(self.X[[colname]])
  #   print('ordinal encoded',self.X[self.cat_colname].nunique())

  def ordinal_encode_cat(self):
    n_unique_series = self.X[self.cat_colname].nunique()
    print('n_unique_series',n_unique_series)
    remaining_cat_colname = list()
    for colname,unique in zip(n_unique_series.index,n_unique_series):
      if unique <= 10:
        oe = OrdinalEncoder()
        self.X[colname] = oe.fit_transform(self.X[[colname]])
      else:
        remaining_cat_colname.append(colname)
      
    print('ordinal encoded',self.X[self.cat_colname].nunique())
    print('Remaining cat cols to be encoded,which has more than 10 uniques ',remaining_cat_colname)
    for remain in remaining_cat_colname:
      for num,cat in enumerate(self.top_ten_cat_col_value[remain],start=1):
        print('cat and num',cat,num)
        self.X[remain] = np.where(self.X[remain] == cat,num,0)
    

  def feature_selection_cat_corr_with_target(self):
    # https://machinelearningmastery.com/feature-selection-with-categorical-data/
    # https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e
    # https://www.kaggle.com/questions-and-answers/55494
    # https://medium.com/analytics-vidhya/categorical-feature-selection-using-chi-squared-test-e4c0d0af6b7e
    # https://datascience.stackexchange.com/questions/10674/dissmissing-features-based-on-correlation-with-target-variable
    # https://towardsdatascience.com/brute-force-variable-selection-techniques-for-classification-problems-5bca328977e5
    cor_with_target_dict = dict()
    to_drop = set()
    for col in self.cat_colname:
      corr = self.X[col].corr(self.y)
      print(f'correlation of feature with target {col} : {corr}')
      if abs(corr) >= abs(.2) or type(corr) ==  type('nan'):
        cor_with_target_dict[col] = corr
      else:
        print('type of col ',type(col))
        to_drop.add(col)
    print('to_drop that are weakly correlated with target',to_drop)
    print('correlated with target above the threshold .2 ',cor_with_target_dict)
    high_corr_with_target = list(cor_with_target_dict.keys())
    print('highly correlated(cat col) with target ',high_corr_with_target)
    return high_corr_with_target

  def feature_selection_cat_mut_with_target(self):
    if(len(self.cat_colname) != 0):
      top_n_mutual_info = list()
      print('////////////////////cat_col',self.cat_colname)
      mutual_info = mutual_info_regression(self.X[self.cat_colname],self.y)
      mutual_info_series = pd.Series(mutual_info)
      mutual_info_series.index = self.cat_colname
      mutual_info_series = mutual_info_series.sort_values(ascending=False)

      if len(mutual_info_series) > 10:
        mutual_info_series = mutual_info_series[:10]
      top_n_mutual_info = mutual_info_series[:self.N_CAT].copy().index  
      #print(mutual_info_series)
      return mutual_info_series,list(top_n_mutual_info)
    return list(),list()

  def select_con_cat(self):
    cont = []
    cat = []
    for feature in self.all_features_selected:
      if feature in self.cat_colname:
        cat.append(feature)
      else:
        cont.append(feature)

    return cat,cont



  # def extra_tree_classifier_cat(self):
  #   if(len(self.cat_colname) != 0):
  #     model = ExtraTreesClassifier()
  #     model.fit(self.X[self.cat_colname],self.y)
  #     arr = model.feature_importances_
  #     feat_imp = {col:val for col,val in zip(self.cat_colname,arr)}
  #     print('feature importance cat',feat_imp)
  #     feat_imp = dict(sorted(feat_imp.items(),key=lambda x: x[1],reverse=True))
  #     print('SORTED cat feature importance',feat_imp)
  #     top_n_extra_tree_classif_cat = list(feat_imp.keys())
  #     top_n_extra_tree_classif_cat = top_n_extra_tree_classif_cat[:self.N_CAT]
  #     print('cat top4_extra_tree_classif',top_n_extra_tree_classif_cat)
  #     return list(top_n_extra_tree_classif_cat)
  #   return list()

  def encoding_and_scaling(self):  
    encoded_cat_feature_colname = []
    cat = sorted(list(self.cat_selected))
    cont = list(self.cont_selected)

    col_transformer = ColumnTransformer(transformers=[('fuly transformed',OneHotEncoder(drop='first'),cat)],remainder=StandardScaler())

    unique_count = []
    for i in cat:
      unique_count.append(self.X[i].nunique())
    print(cat,unique_count)
    for feature_name, counts in zip(cat,unique_count):
      name = [feature_name + str(i+1) for i in range(counts-1)]
      encoded_cat_feature_colname.extend(name)  
    
    column_names = encoded_cat_feature_colname + cont
    print('cat',cat)
    print('cont',cont)
    print('encoded_cat_feature_colname',encoded_cat_feature_colname)
    print('combined all features encoded (HERE EROOR)',column_names)
    return pd.DataFrame(col_transformer.fit_transform(self.X))     # USED obj.x dataframe
    # Shape of passed values is (10617, 1), indices imply (10617, 33)

# The get_best_model() returns a dataframe ,where each row contains the model_name, best_score, best_parameters. 
# And you can get that dataframe by best_model attribute ex: obj.best_model --> returns a dataframe.

class BuildModel:
  def __init__(self, X, y):
    self.X = X
    self.y = y
    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X,y,test_size=.25)
    #self.X.to_csv('fs_titanic.csv')
    #self.y.to_csv('target.csv')

    self.best_model = self.get_best_model(self.X_train,self.y_train)
    print('max accuracy score ','max ',max(self.best_model['best_score']))

    print('the df returned after model build ',self.best_model,self.best_model.best_parameters)

    self.best_model_params = self.best_model.loc[ self.best_model['accuracy_score'] == max(self.best_model['accuracy_score'])]
    print('Best Model Parameters',self.best_model_params)

    print('FINAL RESULT',self.best_model_params.loc[:,['Model_name','best_parameters']])
   
    
    

  def get_best_model(self,X_train,y_train):
  
    #X_train, X_test, y_train, y_test = train_test_split(X,, test_size = .3,random_state=123,stratify=True )
    result_list = []
    params_grid = {
        'svm(SVR)' :{
            'model': SVR(),
            'param':{
            'gamma':['auto','scale'],
            # 'degree':[3,4],
            'C':[1,2,3],
            'kernel':['rbf','linear']
                    }      
            },
        'Random Forest Regressor': {
            'model':RandomForestRegressor(),
            'param':{
            'n_estimators' :[x for x in range(100,400,25)],
            # 'max_depth':[3,4,5,None],
            # 'min_samples_split':[2,3]
                    }
                          },
                   
          'Lasso Regression':{
              'model':Lasso(),
              'param':{
                'alpha':[0.1,0.2,.3,.4,.5,.6,.8]
              }
          },

          'Ridge Regression':{
              'model':Ridge(),
              'param':{
                  'alpha':[0.1,0.2,.3,1.0,1.5,2.0],
              }
          },
          
        'Gradient Boosting Regressor':{
            'model':GradientBoostingRegressor(),
            'param':{
                'n_estimators':[100,120,130,140,150,160,180,200,210,230,250,270],
                'learning_rate':[1.0,2,0.5,0.6,0.8,0.9]
            }
        },

        'AdaBoost Regressor':{
            'model':AdaBoostRegressor(),
            'param':{
                'n_estimators':[50,75,100,120,130,140,150,160,180,200,210,230,250,270],
                'learning_rate':[1.0,2,0.5,0.6,0.8,0.9]
            }
        },

        'Decision Tree Regressor':{
            'model':DecisionTreeRegressor(),
            'param':{
            "splitter":["best","random"],
            "max_depth" : [1,3,5,7,9,11],
            "min_samples_leaf":[1,2,3,4,5,6,7,8,9],
          #  "min_weight_fraction_leaf":[0.1,0.2,0.3,0.4,0.6,0.8,0.9],
           "max_features":["auto","log2","sqrt",None],
           "max_leaf_nodes":[None,10,20,30,40,50,60,70]
            }
        },

        'Linear Regression':{
            'model':LinearRegression(),
            'param':{
                # 'fit_intercept ':[True],
                # 'normalize': [False]

            }
        }

    }
    mean_test_score = 0
    
    for model_name, model_params in params_grid.items() :
      # https://analyticsindiamag.com/guide-to-hyperparameters-tuning-using-gridsearchcv-and-randomizedsearchcv/
      model = RandomizedSearchCV(model_params['model'], model_params['param'], n_iter=2, cv=5, return_train_score=False)
      model.fit(X_train,y_train)
      y_pred = model.predict(self.X_test)
      score = r2_score(self.y_test,y_pred)
      print(f'model name: {model_name} score :{score}')
      # print('log loss',log_loss(self.y_test, y_pred))
      mean_test_score = model.cv_results_['mean_test_score']
      mean_fit_time = model.cv_results_['mean_fit_time']
      result_list.append([model_name, model.best_score_, model.best_params_, mean_test_score, mean_fit_time,score])
    
      # result_list.append({'model_name':model_name,
      #                        'model_score':model.best_score_,
      #                        'model_best_parameter':model.best_params_},ignore_index=True)
     
    return pd.DataFrame(result_list,columns=['Model_name','best_score','best_parameters', 'mean_test_score', 'mean_fit_time','accuracy_score'])

#ob = DataProcess('https://raw.githubusercontent.com/ketangangal/Advance-House-Price-Prediction-/main/Ames_Housing_Data.csv','Regression')

# ob.out_filtered.featureengineeringandselection_regression.model_score.best_model_params

# df = pd.read_csv('https://raw.githubusercontent.com/ketangangal/Advance-House-Price-Prediction-/main/Ames_Housing_Data.csv')

# df.shape

#df.head(2)

#df.dtypes
