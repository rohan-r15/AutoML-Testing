# -*- coding: utf-8 -*-
"""AutoML_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G54zCHQww4WeMV09Y_NBtK35O8MFRbf6

# To import our own module in google colab

##https://colab.research.google.com/github/bebi103a/bebi103a.github.io/blob/master/lessons/03/packages_and_modules.ipynb#scrollTo=ojMnEc6NH2lO
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

import scipy

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import BaggingClassifier
import xgboost as xg
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.naive_bayes import MultinomialNB


from sklearn.metrics import accuracy_score,confusion_matrix,classification_report  #https://medium.com/@MohammedS/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b
from sklearn.metrics import log_loss
from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV
from sklearn.feature_selection import mutual_info_classif
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.feature_selection import VarianceThreshold
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OrdinalEncoder
from sklearn.compose import ColumnTransformer
from sklearn.compose import make_column_transformer
from sklearn.impute import SimpleImputer
from sklearn.experimental import enable_iterative_imputer  # noqa
from sklearn.impute import IterativeImputer
from scipy.stats import shapiro,chi2_contingency
from sklearn.pipeline import make_pipeline

# %matplotlib inline

#pd.set_option('display.max_colwidth',100)
#pd.set_option("display.max_columns", None) 

class DataProcess:
    def __init__(self, df_url, type_of_problem):  # df_url can also be path.
        self.df_url = df_url                      
        self.problem_type =type_of_problem
        self.dataframe = self.get_df(df_url)
        self.nan_threshold()
        self.X,self.y = self.get_X_and_y(self.dataframe)
        le = LabelEncoder()
        self.y = pd.Series(le.fit_transform(self.y),name=self.y.name)
        print(le.classes_,self.y.value_counts())
        self.out_filtered = OutlierRemoval(self.dataframe,self.X,self.y)

    def nan_threshold(self):
      for  col in list(self.dataframe.columns):
        if self.dataframe[col].isnull().sum() > round(self.dataframe.shape[0]*.3):
          print(f'{col} column has more than 30% of nan values so we remove them')
          self.dataframe.drop(columns=col,inplace=True)
        
    def get_df(self,df_url):
        dataset_format = self.df_url[-1:-4:-1]
        dataset_format = dataset_format[::-1]
        try:
            if dataset_format == 'csv':
                return pd.read_csv(df_url)
            elif dataset_format == 'tsv':
                return pd.read_table(df_url)
            elif dataset_format == 'txt':
                return pd.read_table(df_url)
            elif dataset_format == 'xlxs':
                return pd.read_excel(df_url)
            else:
                print('Dataset is not in recognisable format')
                
        except FileNotFoundError:
            print('File Not Found')
    def get_X_and_y(self,dataframe):
      flag=0
      try:
          X = dataframe.drop(dataframe.iloc[:,-1].name,axis='columns')
          y = dataframe.iloc[:,-1]
          print('feature and target separated')
          return X,y
      except :
            flag +=1
            print('Cannot load dataset :(') # If the url, path is invalid or format is not recognizable  .
            exit(0)
      finally:
          if flag == 1:
              X = pd.DataFrame()
              y = pd.DataFrame()
              return X,y

class OutlierRemoval :
  def __init__(self,df,X,y):
    self.df = df
    self.X = X
    self.y = y
    self.cat_features = X.select_dtypes(include='object')
    self.cont_features = X.select_dtypes(exclude='object')
    print('select dtypes cont',self.cont_features.columns)
    print('select dtypes cat',self.cat_features.columns)

    print('raw X with nan ',self.X.isna().sum())
   
    self.X,self.cat_colname,self.cont_colname = self.impute_nan()
    self.new_cat_colname = self.cat_colname.copy()
    print('returned df form impute X null values and nuniques',self.X.isna().sum(),self.X.nunique())
    print('returned df form impute x category column names ',self.cat_colname)
    print('returned df form impute x NEW category column names ',self.new_cat_colname)
    print('returned df form impute x continuous column names ',self.cont_colname)
    
    self.normal_cont_dict,self.not_normal_cont_dict = self.outlier_continuous()
    print('normal_cont_dict',self.normal_cont_dict)
    print('not_normal_cont_dict',self.not_normal_cont_dict)

    self.remove_cont_not_normal_outlier()
    print('after removal of cout_not_normal_outlier',self.df.describe())
    self.remove_cont_not_normal_outlier()

    print('shape of X and y',self.X.shape,self.y.shape)
    print(self.X.isnull().sum())
    print(self.y.isnull().sum())

    self.data = pd.concat([self.X,self.y],axis=1)
    print('Combined X and y as data',self.data.isnull().sum(),self.data.shape)
    print('out cat cols',self.cat_colname) 
    print('out new cat cols',self.new_cat_colname) 
    print('out cont cols',self.cont_colname) 
    print('=======================================================')
    self.featureengineeringandselection = FeatureEngineeringAndSelection(self.X,self.y,self.cat_colname,self.new_cat_colname,self.cont_colname)

  def impute_nan(self): # previously it was X
    cont_col = list(self.cont_features.columns)
    cat_col = list(self.cat_features.columns)
    to_change = []
    for name in cont_col:
      if self.X[name].nunique() < 10:
        if self.X[name].nunique() == 1:
          continue
        to_change.append(name)

    for i in to_change:
      cont_col.remove(i)
    cat_col.extend(to_change)

    ct = make_column_transformer((IterativeImputer(),cont_col),
                         (SimpleImputer(strategy='most_frequent'),cat_col),
                           remainder='passthrough')
    arr = ct.fit_transform(self.X)
    df_X = pd.DataFrame(arr,columns=cont_col+cat_col)
    return df_X,cat_col,cont_col


  def outlier_continuous(self):
    normality_dict = dict()
    not_normality_dict = dict()
    
    for name in self.cont_colname:
      stat, p = shapiro(self.X[name])
      if p > 0.05:
        normality_dict[name] = stat,p
      else:
        not_normality_dict[name] = stat,p
      
    for name in not_normality_dict.keys():   # If a feature is not normally distributed and it contains nuiques < 15, then it is categorical.
      if self.X[name].nunique() < 15:  # these are categorical features
        self.new_cat_colname.extend(name)
        # self.cat_features[name] = self.cont_features[name].copy()
        # self.cont_features.drop(columns=[name],inplace=True)
      
    for var in self.cat_colname:
      if var in not_normality_dict.keys():
        del not_normality_dict[var]

    return normality_dict,not_normality_dict


  def remove_cont_not_normal_outlier(self):
    if (len(self.not_normal_cont_dict) != 0):
      for key,_ in self.not_normal_cont_dict.items():
        Q1 = self.df[key].quantile(.25)
        Q3 = self.df[key].quantile(.75)
        IQR = Q3 - Q1
        lower_limit = Q1 - (1.5 * IQR)
        upper_limit = Q3 + (1.5 * IQR)
        self.df[key] = np.where(self.df[key] > upper_limit, upper_limit,
                                (np.where(self.df[key] < lower_limit,lower_limit,self.df[key]))) 
        
        
  def remove_cont_normal_outlier(self):
    if(len(self.normal_cont_dict) != 0):
      for key,_ in self.normal_cont_dict.items():
        mean_upper_limit = self.cont_features[key].mean() + (3 * self.cont_features[key].std())
        mean_lower_limit = self.cont_features[key].mean() - (3 * self.cont_features[key].std())
        self.df[key] =  self.cont_features[(self.cont_features[key] > mean_lower_limit) & (self.cont_features[key] < mean_upper_limit)]

class FeatureEngineeringAndSelection:
  def __init__(self, X, y, cat_colname, new_cat_colname, cont_colname):
    self.X = X
    self.y = y
    self.cat_colname = cat_colname
    self.new_cat_colname = new_cat_colname
    self.cont_colname = cont_colname
    self.X[self.cont_colname] = self.X[self.cont_colname].astype('float')
    self.zero_variance()
    self.N_CONT = self.select_N(self.cont_colname)
    print('dtypes of X ',self.X.dtypes)
    
    print('After 0 var zero variance drop',self.X.columns)
    print('After 0 var cat column names',self.cat_colname)
    print('After 0 var new cat column names',self.new_cat_colname)
    print('After 0 var cont column names',self.cont_colname)
    print('X column names',self.X.columns)
    self.cor_within_features_to_drop = self.feature_selection_cont_within_features(0.85)
    print('After correlation ',self.X.columns)
    print('After correlation dytpes of X',self.X.dtypes)
    print('returned cor features to drop from correlation',self.cor_within_features_to_drop)
    print('After highly correlated features removed, remainging feature names in cont_colname',self.cont_colname)

    self.mutual_series,self.top_n_mutual_info = self.feature_selection_cont_with_target()
    print('mutual info series returned',self.mutual_series)
    print('top 4 mutual info series returned',self.top_n_mutual_info)

    self.top_4_extra_tree_classif = self.extra_tree_classifier_cont()
    self.high_corr_with_target = self.feature_selection_cont_cor_with_target()
    print('cont correlation with target',self.high_corr_with_target)

    self.top_ten_cat_col_value = self.choose_top_ten_cat_values()
    #print(self.encoding_and_scaling())

    print('FINALLY, BEFORE CONT FEATURE SELECTION X IS',self.X[self.cont_colname].columns)
    print('highly correlated with target ',self.high_corr_with_target,type(self.high_corr_with_target))
    print(f'top {self.N_CONT} mutual info classif series ',self.top_n_mutual_info,type(self.top_n_mutual_info))
    print(f'top {self.N_CONT} extra tress clasifier feature importance ',self.top_4_extra_tree_classif,type(self.top_4_extra_tree_classif))

    self.all_cont_features_selected = ((self.high_corr_with_target + self.top_n_mutual_info) + self.top_4_extra_tree_classif)
    print('All cont features selected ',self.all_cont_features_selected)

    self.final_cont_feature_to_consider = set(self.all_cont_features_selected)
    print('final cont features selected ',self.final_cont_feature_to_consider)
    print(' N   VALUE = ',self.N_CONT)
    print(' NO OF CONT COLUMNS = ',len(self.cont_colname))
    print('remaining cat columns which are to be feature selected ',self.cat_colname)
    print('FINALLY, BEFORE CONT FEATURE SELECTION X IS',self.X[self.cont_colname].columns)
    print('FINALLY, After CONT FEATURE SELECTION X IS',self.X[self.final_cont_feature_to_consider].columns)
    self.ordinal_encode_cat()
    self.N_CAT = self.select_N(self.cat_colname)
    print('cat N ',self.N_CAT)
    #self.full_ordinal_encode_cat()
    # print(self.X['Name'])
    self.cat_highly_correlated = self.feature_selection_cat_corr_with_target()
    self.mutual_series_cat,self.top_n_mutual_info_cat = self.feature_selection_cat_with_target()
    self.top_n_extra_tree_classif_cat = self.extra_tree_classifier_cat()

    print('mutual info series cat returned',self.mutual_series_cat)
    print('top 4 mutual info series cat returned',self.top_n_mutual_info_cat)
    print('top n extra tree classif cat ',self.top_n_extra_tree_classif_cat)
    self.all_cat_features_to_consider = set(self.cat_highly_correlated + self.top_n_mutual_info_cat + self.top_n_extra_tree_classif_cat)
    print('all cat features to consider ',self.all_cat_features_to_consider)
    self.all_feature_selected = list(self.final_cont_feature_to_consider) + list(self.all_cat_features_to_consider)
    print('So ALL FEATURE SELECTED COLUMNS ',self.all_feature_selected)

    print('all cont features before feature selection',self.cont_colname)
    print('all cat features before feature selection',self.cat_colname)

    self.cat_selected, self.cont_selected = self.select_con_cat() 
    print(f'cat selected {self.cat_selected} , cont selected {self.cont_selected}')
    print('target',self.y.name)

    self.X_scaled = self.encoding_and_scaling()
    print('X_scaled columns',self.X_scaled.columns)
    print(self.X_scaled.head(2))

    self.model_score = BuildModel(self.X_scaled,self.y)
    print(self.model_score.best_model)

  def select_N(self,colname):
    N = 0
    if len(colname) <= 3:
      N = 1
    elif len(colname) <= 5:
      N = 2
    elif len(colname) <=10:
      N = 4
    elif len(colname) <= 15:
      N = 6
    elif len(colname) <= 20:
      N = 10
    else:
      N = 15
    return N

  def zero_variance(self):
    if len(self.cont_colname) != 0:
      var_threshold = VarianceThreshold(threshold=0)
      var_threshold.fit_transform(self.X[self.cont_colname])
      drop_features = self.X[self.cont_colname].columns[~var_threshold.get_support()]  #df[cont.columns].columns[~var.get_support()]
      print('zero_variance columns to drop',drop_features)

      if len(drop_features) != 0:
        self.X.drop(columns=drop_features,inplace=True)
        print('inside if drop_feature')
        print('zero var feat',drop_features)
        for i in drop_features:
          self.cont_colname.remove(i)
      print('After drop zero variance col, remaining cols in cont are',self.cont_colname)
      


  # using correlation
  def feature_selection_cont_within_features(self,thershold):
    cor_matrix = self.X.corr(method='pearson')
    cor_features_drop = set()
    for i in range(len(cor_matrix.columns)):
      for j in range(i):
        if abs(cor_matrix.iloc[i,j]) > thershold:
          cor_features_drop.add(cor_matrix.columns[i]) 
    print('Highly correlated features',cor_features_drop)

    if len(cor_features_drop) != 0:
      self.X.drop(columns=cor_features_drop,inplace=True)
      for col in cor_features_drop:
        print('cont name to be removed form cont_colname',col)
        self.cont_colname.remove(col)
     
    return cor_features_drop


  # using mutual info classif
  def feature_selection_cont_with_target(self):
    top_n_mutual_info = list()
    print('////////////////////cont_col',self.cont_colname)
    mutual_info = mutual_info_classif(self.X[self.cont_colname],self.y)
    mutual_info_series = pd.Series(mutual_info)
    mutual_info_series.index = self.cont_colname
    mutual_info_series = mutual_info_series.sort_values(ascending=False)

    if len(mutual_info_series) > 10:
      mutual_info_series = mutual_info_series[:10]
    top_n_mutual_info = mutual_info_series[:self.N_CONT].copy().index  
    return mutual_info_series,list(top_n_mutual_info)


  # using correlation of cont with target
  def feature_selection_cont_cor_with_target(self):
    cor_with_target_dict = dict()
    to_drop = set()
    for col in self.cont_colname:
      corr = self.X[col].corr(self.y)
      print(f'correlation of feature with target {col} : {corr}')
      if corr >= abs(.2):
        cor_with_target_dict[col] = corr
      else:
        to_drop.add(col)
    print('to_drop that are weakly correlated with target',to_drop)
    print('correlated with target above the threshold .2 ',cor_with_target_dict)
    high_corr_with_target = list(cor_with_target_dict.keys())
    print('highly correlated with target ',high_corr_with_target)
    # if len(to_drop) != 0:
    #   for i in to_drop:
    #     self.X.drop(columns=i,inplace=True)
    #     self.cont_colname.remove(i)

    return high_corr_with_target


  # for cont features
  def extra_tree_classifier_cont(self):
    model = ExtraTreesClassifier()
    model.fit(self.X[self.cont_colname],self.y)
    arr = model.feature_importances_
    feat_imp = {col:val for col,val in zip(self.cont_colname,arr)}
    print('feature importance',feat_imp)
    feat_imp = dict(sorted(feat_imp.items(),key=lambda x: x[1],reverse=True))
    print('SORTED feature importance',feat_imp)
    top_n_extra_tree_classif = list(feat_imp.keys())
    top_n_extra_tree_classif = top_n_extra_tree_classif[:self.N_CONT]
    print('top4_extra_tree_classif',top_n_extra_tree_classif)
    return top_n_extra_tree_classif

    # print('feature importance array------',sorted(arr,reverse=True))

  def choose_top_ten_cat_values(self):
    top_ten_cat_col_value = dict()
    print('cat colnames choose top ten from each',self.cat_colname)
    for col in self.X[self.cat_colname]:
      print(col,' : ',len(self.X[col].unique()),'label')

    for col in self.cat_colname:
      top_ten = [x for x in self.X[col].value_counts().sort_values(ascending=False).head(10).index]
      print(f'top 10 for {col} ',top_ten)
      top_ten_cat_col_value[col] = top_ten
    print(top_ten_cat_col_value)
    return top_ten_cat_col_value

  
  def ordinal_encode_cat(self):
    n_unique_series = self.X[self.cat_colname].nunique()
    print('n_unique_series',n_unique_series)
    remaining_cat_colname = list()
    for colname,unique in zip(n_unique_series.index,n_unique_series):
      if unique <= 10:
        oe = OrdinalEncoder()
        self.X[colname] = oe.fit_transform(self.X[[colname]])
      else:
        remaining_cat_colname.append(colname)
      
    print('ordinal encoded',self.X[self.cat_colname].nunique())
    print('Remaining cat cols to be encoded,which has more than 10 uniques ',remaining_cat_colname)
    for remain in remaining_cat_colname:
      for num,cat in enumerate(self.top_ten_cat_col_value[remain],start=1):
        print('cat and num',cat,num)
        self.X[remain] = np.where(self.X[remain] == cat,num,0)
    
    
      # # np.where(top_ten_cat_col_value[remain].isin[self.X[remain]], )
      # print(self.top_ten_cat_col_value[remain])
      # isin = self.X[remain].isin(self.top_ten_cat_col_value[remain])
      # print('isin condintion check ',self.X[remain].isin(self.top_ten_cat_col_value[remain]))
      # print(isin.value_counts())

      # n=pd.Series(np.where(self.X[remain].isin(self.top_ten_cat_col_value[remain]),self.X[remain],0  ))
      # print('unique and valuecounts',n.unique(),n.value_counts())
      # print(n)

  # By using mutual info classif for cat
  def feature_selection_cat_with_target(self):
    top_n_mutual_info = list()
    if (len(self.cat_colname) != 0):
      print('////////////////////cont_col',self.cat_colname)
      mutual_info = mutual_info_classif(self.X[self.cat_colname],self.y)
      mutual_info_series = pd.Series(mutual_info)
      mutual_info_series.index = self.cat_colname
      mutual_info_series = mutual_info_series.sort_values(ascending=False)

      if len(mutual_info_series) > 10:
        mutual_info_series = mutual_info_series[:10]
      top_n_mutual_info = mutual_info_series[:self.N_CAT].copy().index  
      #print(mutual_info_series)
      return mutual_info_series,list(top_n_mutual_info)
    return list(),list()


  
    
  def full_ordinal_encode_cat(self):
    for col in self.X[self.cat_colname]:
      oe = OrdinalEncoder()
      self.X[col] = oe.fit_transform(self.X[[col]])
      



  
    # https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection
    # https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/
    # https://machinelearningmastery.com/feature-selection-with-categorical-data/
    # https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e
    # https://www.kaggle.com/questions-and-answers/55494
    # https://medium.com/analytics-vidhya/categorical-feature-selection-using-chi-squared-test-e4c0d0af6b7e
    # https://datascience.stackexchange.com/questions/10674/dissmissing-features-based-on-correlation-with-target-variable
    # https://towardsdatascience.com/brute-force-variable-selection-techniques-for-classification-problems-5bca328977e5
    # f_scores_dict = dict()
    # model= SelectKBest(score_func=chi2, k='all')
    # model.fit_transform(self.X[self.cat_colname],self.y)
    # for colname,fscore in zip(self.cat_colname, model.scores_):
    #   f_scores_dict[colname] = fscore
    # f_scores_dict = dict(sorted(f_scores_dict.items(),key= lambda x: x[1],reverse=True))
    # print('SORTED f_scores_dict',f_scores_dict)
    # print('f_scores',model.scores_)
    # print('f_scores_dict',f_scores_dict)

  def feature_selection_cat_corr_with_target(self):
    cor_with_target_dict = dict()
    to_drop = set()
    for col in self.cat_colname:
      corr = self.X[col].corr(self.y)
      print(f'correlation of feature with target {col} : {corr}')
      if abs(corr) >= abs(.2):
        cor_with_target_dict[col] = corr
      else:
        to_drop.add(col)
    print('to_drop that are weakly correlated with target',to_drop)
    print('correlated with target above the threshold .2 ',cor_with_target_dict)
    high_corr_with_target = list(cor_with_target_dict.keys())
    print('highly correlated(cat col) with target ',high_corr_with_target)
    return high_corr_with_target

  # for cat features
  def extra_tree_classifier_cat(self):
    if(len(self.cat_colname) != 0):
      model = ExtraTreesClassifier()
      model.fit(self.X[self.cat_colname],self.y)
      arr = model.feature_importances_
      feat_imp = {col:val for col,val in zip(self.cat_colname,arr)}
      print('feature importance cat',feat_imp)
      feat_imp = dict(sorted(feat_imp.items(),key=lambda x: x[1],reverse=True))
      print('SORTED cat feature importance',feat_imp)
      top_n_extra_tree_classif_cat = list(feat_imp.keys())
      top_n_extra_tree_classif_cat = top_n_extra_tree_classif_cat[:self.N_CAT]
      print('cat top4_extra_tree_classif',top_n_extra_tree_classif_cat)
      return list(top_n_extra_tree_classif_cat)
    return list()  

  def select_con_cat(self):
    cont = []
    cat = []
    for feature in self.all_feature_selected:
      if feature in self.cat_colname:
        cat.append(feature)
      else:
        cont.append(feature)

    return cat,cont

  def encoding_and_scaling(self):  
    encoded_cat_feature_colname = []
    cat = sorted(list(self.cat_selected))
    cont = list(self.cont_selected)

    col_transformer = ColumnTransformer(transformers=[('fuly transformed',OneHotEncoder(drop='first'),cat)],remainder=StandardScaler())

    unique_count = []
    for i in cat:
      unique_count.append(self.X[i].nunique())
    print(cat,unique_count)
    for feature_name, counts in zip(cat,unique_count):
      name = [feature_name + str(i+1) for i in range(counts-1)]
      encoded_cat_feature_colname.extend(name)  
    
    column_names = encoded_cat_feature_colname + cont
    print('cat',cat)
    print('cont',cont)
    print('encoded_cat_feature_colname',encoded_cat_feature_colname)
    print('combined all features encoded (HERE EROOR)',column_names)
    return pd.DataFrame(col_transformer.fit_transform(self.X))     # USED obj.x dataframe
    # Shape of passed values is (10617, 1), indices imply (10617, 33)

# The get_best_model() returns a dataframe ,where each row contains the model_name, best_score, best_parameters. 
# And you can get that dataframe by best_model attribute ex: obj.best_model --> returns a dataframe.

class BuildModel:
  def __init__(self, X, y):
    self.X = X
    self.y = y
    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X,y,test_size=.25,stratify=y)
    #self.X.to_csv('fs_titanic.csv')
    #self.y.to_csv('target.csv')

    self.best_model = self.get_best_model(self.X_train,self.y_train)
    print('max accuracy score ','max ',max(self.best_model['accuracy_score']))
    
    self.best_model_params = self.best_model.loc[ self.best_model['accuracy_score'] == max(self.best_model['accuracy_score'])]
    print('Best Model Parameters',self.best_model_params)

    print('FINAL RESULT',self.best_model_params.loc[:,['Model_name','best_parameters']])


    
   
    
    

  def get_best_model(self,X_train,y_train):
  
    #X_train, X_test, y_train, y_test = train_test_split(X,, test_size = .3,random_state=123,stratify=True )
    result_list = []
    params_grid = {
        'svm' :{
            'model': SVC(),
            'param':{
            'gamma':['auto','scale'],
            # 'degree':[3,4],
            'C':[1,2,3],
            'kernel':['rbf','linear']
                    }      
            },
        'Random Forest': {
            'model':RandomForestClassifier(),
            'param':{
            'n_estimators' :[x for x in range(100,375,25)],
            # 'max_depth':[3,4,5,None],
            # 'min_samples_split':[2,3]
                    }
                          },
        # 'Multinomial NB':{
        #     'model': MultinomialNB(),
        #     'param':{
        #         'alpha':scipy.stats.uniform(scale=1)
        #     }
        # },
        'Ada Boost':{
            'model':AdaBoostClassifier(),
            'param':{
                'n_estimators':[x for x in range(100,275,25)],
                'learning_rate':[1.0,2,0.3,0.5,0.6,0.8]
            }
        },
        'Gradient Boost':{
            'model':GradientBoostingClassifier(),
            'param':{
                'n_estimators':[100,120,130,140,150,160,180,200,210,230,250],
                'learning_rate':[1.0,2,0.5,0.6,0.8,0.9]
            }
        },
        'XG Boost':{
            'model':xg.XGBClassifier(),
            'param':{
                'n_estimators':[x for x in range(75,376,25)],
                'learning_rate':[.1,.2,.3,.4,.5],
                'max_depth':[3,4,5]
            }
        },
        'KNN':{# https://medium.datadriveninvestor.com/k-nearest-neighbors-in-python-hyperparameters-tuning-716734bc557f
            'model':KNeighborsClassifier(),
            'param':{
                'n_neighbors':[x for x in range(5,15,2)],
                'leaf_size':[30,33,36,39]
            }
        },
        'Bagging Classifier Decision Tree':{
            'model':BaggingClassifier(base_estimator=DecisionTreeClassifier()),
            'param':{
                  'n_estimators':[x for x in range(100,300,25)]
            }
        },
        #  'Bagging Classifier KNN ':{
        #      'model':BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3)),
        #       'param':{
        #           'n_estimators': [x for x in range(150,400,35)]
        #       }
        # },
         'Gaussian NB':{
             'model':GaussianNB(),                           
             'param':{
                 'var_smoothing':[1e-9]
             }
         } 
    }
    mean_test_score = 0
    
    for model_name, model_params in params_grid.items() :
      # https://analyticsindiamag.com/guide-to-hyperparameters-tuning-using-gridsearchcv-and-randomizedsearchcv/
      model = RandomizedSearchCV(model_params['model'], model_params['param'], n_iter=2, cv=5, return_train_score=False, scoring='accuracy')
      model.fit(X_train,y_train)
      y_pred = model.predict(self.X_test)
      score = accuracy_score(self.y_test,y_pred)
      print(f'model name: {model_name} score :{score}')
     
      mean_test_score = model.cv_results_['mean_test_score']
      mean_fit_time = model.cv_results_['mean_fit_time']
      result_list.append([model_name, model.best_score_, model.best_params_, mean_test_score, mean_fit_time,score])
    
      # result_list.append({'model_name':model_name,
      #                        'model_score':model.best_score_,
      #                        'model_best_parameter':model.best_params_},ignore_index=True)
     
    return pd.DataFrame(result_list,columns=['Model_name','best_score','best_parameters', 'mean_test_score', 'mean_fit_time','accuracy_score'])

# ob = DataProcess('https://raw.githubusercontent.com/pplonski/datasets-for-start/master/digits/train.csv','classification')

# ob.out_filtered.featureengineeringandselection.model_score.best_model_params

# ob.X

# dataset_dataset = pd.read_csv('https://raw.githubusercontent.com/ketangangal/HeartDiseaseEstimator/master/csvFiles/Feature_correction.csv')
# df_2=pd.get_dummies(dataset_dataset.drop(columns=['target']),drop_first=True)
# X = df_2
# y = dataset_dataset.target
# obj = BuildModel(X,y)
# obj.best_model
# print(obj.best_model['accuracy_score'])

# obj.best_model

# best= obj.best_model.loc[ obj.best_model['accuracy_score'] == max(obj.best_model['accuracy_score'])]
# best

# best.loc[:,['Model_name','best_parameters']]







type('corr') ==  type('nan')